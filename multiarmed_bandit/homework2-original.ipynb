{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STA 220 Homework 2\n",
    "===========\n",
    "\n",
    "- Do not distribute\n",
    "- Do the entire homework here in the notebook by adding cells below the given exercise.\n",
    "- When turning the homework in simply submit your notebook file to canvas.\n",
    "- Obviously, do not copy the code from some online source or the other students.\n",
    "- After finishing the homwork, please select Kernel -> Restart & Run All, then save, submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-armed bandit framework assumes that you are in a two player game where you select a number from 1 to K (we call this number the arm that you pulled).  Then the other player selects a reward, $r_{i,t}$ based on the arm, $i$, at time $t$ that you selected and reveals that to you.  Your job is to come up with a policy which determines which arm to pull at a given time based on the past performances of the arms.\n",
    "\n",
    "The name multi-armed bandit comes from the gambling world, in which a slot machine is called a one armed bandit.  In that setting, you pull the arm and recieve some reward.  In this fictional setting there are multiple arms for the slot machine, each paying out different rewards.  Because you can only pull one at a time, you only see the reward from the arm you pulled.  This partial observability puts you in the challenging position of needing to explore the arms, seeing which has better performance, before you start to exploit the best arm.  Below is a simulation from a simple mult-armed bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBandit:\n",
    "    '''\n",
    "    The bandit class you will use in this homework. DO NOT modify\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._mu = np.array([-1.,-2.,1.5,0.5,-0.25,.75,.1,1.8,-3])\n",
    "        self._p = 1 / (1 + np.exp(-self._mu))\n",
    "        self.num_arms = len(self._mu)\n",
    "        self.total_rewards = np.zeros(len(self._mu))\n",
    "        \n",
    "    def pull(self,arms):\n",
    "        self.current_rewards = np.random.binomial(1,self._p)\n",
    "        self.total_rewards += self.current_rewards\n",
    "        return self.current_rewards[arms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "band = SimpleBandit()\n",
    "[band.pull(1) for t in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we see that if we had pulled the 1 arm 10 times then our total reward for that arm is 2 because it returned a 1 twice.  Our rewards are binary, only 0 or 1.  You can also see the total rewards from each arm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band.total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple policy is to always pull the arm 2, and another is to pull the arm 1.  We can compare these with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([band.pull([1,2]) for t in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewards now has the rewards for each policy in each column, we can compare the rewards for these policies below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 'always pull 2' is a better policy.  Another simple policy is to randomly select an arm and pull it.  This can be seen as a pure exploration policy.  All of your policies should have the select_arm method which tells you which arm to pull, and the update_reward method which updates any internal state information based on the observed reward.  In this case nothing needs to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    \"\"\"\n",
    "    Random policy, pure exploration. DO NOT modify\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.current_arm = None\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"\n",
    "        choose which arm to pull\n",
    "        \"\"\"\n",
    "        self.current_arm = np.random.randint(self.num_arms)\n",
    "        return self.current_arm\n",
    "    \n",
    "    def update_reward(self, reward):\n",
    "        \"\"\"\n",
    "        enter observed reward\n",
    "        \"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** The regret of a bandit policy is the difference between the total reward you would get from the \"best\" arm in hindsight and the total reward your policy recieved.  So if your policy selected arms $i_1,\\ldots,i_T$ for total of $T$ time, then the regret is \n",
    "$$\\max_j \\sum_{t=1}^T r_{j,t} - \\sum_{t=1}^T r_{i_t,t}.$$\n",
    "The SimpleBandit class maintains what reward you would have recieved if you just pulled a given arm in the ``total_rewards`` attribute (`current_rewards[i]` is the total reward recieved if i is pulled each time).  Fill the def below which takes a list of policies to play over T time steps and returns a list of regrets for each policy.  Test it on the simple bandit and random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trajectory(bandit, policies, T):\n",
    "    \"\"\"\n",
    "    Run T steps of bandit pulling each policy in each time step\n",
    "    \n",
    "    Arguments:\n",
    "    bandit: \n",
    "        a fresh instance of a Bandit class, \n",
    "        in this homework you will be always using an instance from RandomPolicy class\n",
    "    \n",
    "    policies:\n",
    "        a list like [policy1, policy2, policy3 ...]\n",
    "        each of the policy will have select_arm method and update_reward method\n",
    "    \n",
    "    Output: \n",
    "        regret of each policy in list, like\n",
    "        [regret1, regret2, ...]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code, do not change\n",
    "t1 = time.time()\n",
    "regrets = [run_trajectory(SimpleBandit(), [RandomPolicy()], 1000) for _ in range(10)]\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** epsilon-greedy is a policy that has a few roughly equivalent variants, but the one we will use is the following:  For each time step, with probability epsilon, pull an arm at random, otherwise pull the current best arm.  The current best arm is the one which has the most total reward up to that time.  If there is a tie for current best policy, break the tie randomly.\n",
    "\n",
    "Implement epsilon-greedy in the following class, test it by having it compete once with the random policy for epsilon=0.1 for T = 1000 time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the class below\n",
    "class Epsilon_Greedy_Policy:\n",
    "    \n",
    "    def __init__(self, num_arms, eps):\n",
    "        self.num_arms = num_arms\n",
    "        self.current_arm = None\n",
    "        \n",
    "    def select_arm(self):\n",
    "        None\n",
    "        \n",
    "    def update_reward(self, reward):\n",
    "        None\n",
    "        \n",
    "    # more methods if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "t1 = time.time()\n",
    "regrets = []\n",
    "\n",
    "# Your code here, \n",
    "    # generate 10 regrets with simple bandit and epsilon-greedy policy, \n",
    "    # save results in regrets,\n",
    "    # refer to second block in exercise 1\n",
    "\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Exp3 is a policy that is more nuanced.  The idea is that at each time point you pull an arm with some probability `pi[i]` which is updated based on the performance of the arm.  Look at the full algorithm in http://proceedings.mlr.press/v24/seldin12a/seldin12a.pdf, Algorithm 1.  \n",
    "\n",
    "Implement this version of the Exp3 algorithm in the following class, test it by having it compete once with the random policy for T = 1000 time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the class below\n",
    "class Exp3:\n",
    "    \n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.current_arm = None\n",
    "        \n",
    "    def select_arm(self):\n",
    "        None\n",
    "        \n",
    "    def update_reward(self, reward):\n",
    "        None\n",
    "        \n",
    "    # more methods if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "t1 = time.time()\n",
    "regrets = []\n",
    "\n",
    "# Your code here, \n",
    "    # generate 10 regrets with simple bandit and Exp3 policy, \n",
    "    # save results in regrets,\n",
    "    # refer to second block in exercise 1\n",
    "\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** The Upper Confidence Bound (UCB) algorithm is intuitive.  You pull the arm that has the largest upper confidence bound for the mean reward.  So if you have an upper bound on the mean reward for an arm this is either because the arm is performing well or you have not pulled the arm much, resulting in a wide confidence interval.  The UCB for arm i is then,\n",
    "$$ U_{i,t} = \\hat \\mu_{i,t} + C \\sqrt{\\frac{\\log(t)}{n_{i,t} + 1}}$$\n",
    "where $\\hat \\mu_{i,t}$ is the mean reward of the arm up to time $t$, $n_{i,t}$ is the number of times that that arm has been pulled.  $C$ is an argument (can be taken to be 2 by default).\n",
    "\n",
    "Implement this version of the UCB algorithm in the following class, test it by having it compete once with the random policy for T = 1000 time points with $C = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the class below\n",
    "class UCB:\n",
    "    \n",
    "    def __init__(self, num_arms, C):\n",
    "        self.num_arms = num_arms\n",
    "        self.current_arm = None\n",
    "        \n",
    "    def select_arm(self):\n",
    "        None\n",
    "        \n",
    "    def update_reward(self, reward):\n",
    "        None\n",
    "        \n",
    "    # more methods if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "t1 = time.time()\n",
    "regrets = []\n",
    "\n",
    "# Your code here, \n",
    "    # generate 10 regrets with simple bandit and UCB policy, \n",
    "    # save results in regrets,\n",
    "    # refer to second block in exercise 1\n",
    "\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** \n",
    "\n",
    "1. Modify the `run_trajectory` def to output all of the regrets up to that time in a T x K (for K policies) array.\n",
    "2. Try 4 different values of $\\epsilon$ for epsilon-greedy: 0.01, 0.05, 0.1, 0.2 and have them compete.  Plot the regrets as a function of t.  Note the best performing selection of $\\epsilon$ at $T = 1000$.\n",
    "3. Try 4 different values of $C$ in UCB: 0.5, 0.75, 1, 2 and have them compete.  Plot the regrets as a function of t.  Note the best performing selection of $C$ at $T = 1000$.\n",
    "4. Using the optimal values of $C$ and $\\epsilon$ have all four methods compete. Plot the regrets as a function of t, remark on if the relative performance changes over time.  Is one algorithm always dominant?  Make any other conclusions.\n",
    "\n",
    "**Note**: \n",
    "  + Please finishe 4 parts in 4 code blocks.\n",
    "  + For part 2, 3, and 4 you should call `run_trajectory` in each of them. In the end of each part, you need to write a markdown block to explain your findings and state your conclusion.\n",
    "  + For each part, set figure size to (17, 8), plot all lines in the same graph and set proper title. DO NOT create more than 1 subplots. Include legend in your plots so that they are easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1. Write run_trajectory function here again, do not modify the above block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 code, output your plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings and conclusion for part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 code, output your plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings and conclusion for part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 code, output your plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings and conclusion for part 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
