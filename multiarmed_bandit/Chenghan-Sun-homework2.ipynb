{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STA 220 Homework 2\n",
    "===========\n",
    "\n",
    "- Do not distribute\n",
    "- Do the entire homework here in the notebook by adding cells below the given exercise.\n",
    "- When turning the homework in simply submit your notebook file to canvas.\n",
    "- Obviously, do not copy the code from some online source or the other students.\n",
    "- After finishing the homwork, please select Kernel -> Restart & Run All, then save, submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Name: Chenghan Sun\n",
    "\n",
    "## Your Student ID: 915030521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-armed bandit framework assumes that you are in a two player game where you select a number from 1 to K (we call this number the arm that you pulled).  Then the other player selects a reward, $r_{i,t}$ based on the arm, $i$, at time $t$ that you selected and reveals that to you.  Your job is to come up with a policy which determines which arm to pull at a given time based on the past performances of the arms.\n",
    "\n",
    "The name multi-armed bandit comes from the gambling world, in which a slot machine is called a one armed bandit.  In that setting, you pull the arm and recieve some reward.  In this fictional setting there are multiple arms for the slot machine, each paying out different rewards.  Because you can only pull one at a time, you only see the reward from the arm you pulled.  This partial observability puts you in the challenging position of needing to explore the arms, seeing which has better performance, before you start to exploit the best arm.  Below is a simulation from a simple mult-armed bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBandit:\n",
    "    '''\n",
    "    The bandit class you will use in this homework. DO NOT modify\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._mu = np.array([-1.,-2.,1.5,0.5,-0.25,.75,.1,1.8,-3])\n",
    "        self._p = 1 / (1 + np.exp(-self._mu))\n",
    "        self.num_arms = len(self._mu)\n",
    "        self.total_rewards = np.zeros(len(self._mu))\n",
    "        \n",
    "    def pull(self,arms):\n",
    "        self.current_rewards = np.random.binomial(1,self._p)\n",
    "        self.total_rewards += self.current_rewards\n",
    "        return self.current_rewards[arms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "band = SimpleBandit()  # reset\n",
    "[band.pull(2) for t in range(10)]  # do arm 1 for 10 times \n",
    "# band.pull(2)  # set one arm and only pull once "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we see that if we had pulled the 1 arm 10 times then our total reward for that arm is 2 because it returned a 1 twice.  Our rewards are binary, only 0 or 1.  You can also see the total rewards from each arm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 9., 6., 2., 6., 3., 8., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "band.total_rewards  # for each arm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 1 0 1 0]\n",
      "[2. 2. 9. 6. 2. 6. 3. 8. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(band.current_rewards)  # current rewards list for each arm \n",
    "print(band.total_rewards)  # total rewards for each arm w.r.t 10(above) time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple policy is to always pull the arm 2, and another is to pull the arm 1.  We can compare these with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([band.pull([1,2]) for t in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewards now has the rewards for each policy in each column, we can compare the rewards for these policies below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 90])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.sum(axis=0)  # first two arms, 100 time steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 'always pull 2' is a better policy.  Another simple policy is to randomly select an arm and pull it.  This can be seen as a pure exploration policy.  All of your policies should have the select_arm method which tells you which arm to pull, and the update_reward method which updates any internal state information based on the observed reward.  In this case nothing needs to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    \"\"\"\n",
    "    Random policy, pure exploration. DO NOT modify\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.current_arm = None\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"\n",
    "        choose which arm to pull\n",
    "        \"\"\"\n",
    "        self.current_arm = np.random.randint(self.num_arms)\n",
    "        return self.current_arm\n",
    "    \n",
    "    def update_reward(self, reward):\n",
    "        \"\"\"\n",
    "        enter observed reward\n",
    "        \"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** The regret of a bandit policy is the difference between the total reward you would get from the \"best\" arm in hindsight and the total reward your policy recieved.  So if your policy selected arms $i_1,\\ldots,i_T$ for total of $T$ time, then the regret is \n",
    "$$\\max_j \\sum_{t=1}^T r_{j,t} - \\sum_{t=1}^T r_{i_t,t}.$$\n",
    "The SimpleBandit class maintains what reward you would have recieved if you just pulled a given arm in the ``total_rewards`` attribute (`current_rewards[i]` is the total reward recieved if i is pulled each time).  Fill the def below which takes a list of policies to play over T time steps and returns a list of regrets for each policy.  Test it on the simple bandit and random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For t in len(T):\n",
    "    # use rand(9)\n",
    "    # do multiple policy choice on rand(9), which gives list of n arms randomly\n",
    "    # input the list of arms to the bandit(), which returned the total rewards and current rewards\n",
    "    # do the same subtraction to find regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trajectory(bandit, policies, T):\n",
    "    \"\"\"\n",
    "    Run T steps of bandit pulling each policy in each time step\n",
    "    \n",
    "    Arguments: \n",
    "    bandit: \n",
    "        a fresh instance of a Bandit class, \n",
    "        in this homework you will be always using an instance from RandomPolicy class\n",
    "    \n",
    "    policies:\n",
    "        a list like [policy1, policy2, policy3 ...]\n",
    "        each of the policy will have select_arm method and update_reward method\n",
    "    \n",
    "    Output: \n",
    "        regret of each policy in list, like\n",
    "        [regret1, regret2, ...]\n",
    "    \"\"\"\n",
    "    arms_cumulative_rewards = np.zeros(len(policies))  # initialize the policy space \n",
    "    for t in range(T):  # iterate all time steps\n",
    "        selected_arms = [p.select_arm() for p in policies]  # select random arm T times under each policy\n",
    "        # print(f\"Time step {t} selects these arms: {selected_arms} under each policy\")\n",
    "        arms_current_rewards = bandit.pull(selected_arms)  # pull, pass to policy class\n",
    "        \n",
    "        global CurRewardList_EGP\n",
    "        global CurRewardList_UCB\n",
    "        if 'Epsilon_Greedy_Policy' in str(policies):\n",
    "            CurRewardList_EGP.append(arms_current_rewards[1])  # append to global scope \n",
    "        elif 'UCB' in str(policies):\n",
    "            CurRewardList_UCB.append(arms_current_rewards[1])  # append to global scope \n",
    "        arms_cumulative_rewards += arms_current_rewards\n",
    "        # print(f\"The current binary rewards for the {len(selected_arms)} arms: {arms_current_rewards}\")\n",
    "        # print(f\"The cumulative rewards at {t} time step are: {arms_cumulative_rewards}\")\n",
    "        # print(f\"The potential total rewards for each possible arm: {bandit.total_rewards}\")\n",
    "        regrets_history = [np.amax(bandit.total_rewards) - cu_reward for cu_reward in arms_cumulative_rewards]\n",
    "        # print(f\"The regrets history at the {t}th time step for the {len(policies)} arms: \\033[91m{regrets_history}\\033[0m\")\n",
    "    return regrets_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Usage : 0.45552587509155273\n",
      "[[375.0], [345.0], [350.0], [353.0], [397.0], [347.0], [366.0], [366.0], [383.0], [385.0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "366.7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code, do not change\n",
    "t1 = time.time()\n",
    "num_arms = SimpleBandit().num_arms\n",
    "p = RandomPolicy(num_arms)\n",
    "policies = [p]\n",
    "regrets = [run_trajectory(SimpleBandit(), policies, 1000) for _ in range(10)]\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "print(regrets)\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** epsilon-greedy is a policy that has a few roughly equivalent variants, but the one we will use is the following:  For each time step, with probability epsilon, pull an arm at random, otherwise pull the current best arm.  The current best arm is the one which has the most **mean** reward up to that time.  If there is a tie for current best policy, break the tie randomly.\n",
    "\n",
    "Implement epsilon-greedy in the following class, test it by having it compete once with the random policy for epsilon=0.1 for T = 1000 time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epsilon_Greedy_Policy:\n",
    "    \"\"\" \n",
    "    Epsilon-greedy algorithm:\n",
    "        With probability epsilon, pull an arm at random, otherwise pull the current best arm.\n",
    "        The current best arm is the one which has the most total reward up to that time\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms, eps):\n",
    "        self.num_arms = num_arms  # total number of arms \n",
    "        self.current_arm = None\n",
    "        self.eps = eps  # probability epsilon for decision making \n",
    "        self.t = 0  # initialized time step \n",
    "        self.cumulative_rewards = np.zeros(self.num_arms)\n",
    "        self.arms_counter = np.zeros(self.num_arms)\n",
    "        self.arms_performance = np.zeros(self.num_arms)  # update arms mean rewards dynamically \n",
    "        \n",
    "    def greedy_mode(self):\n",
    "        \"\"\" Greedily policy: \n",
    "                decision made dynamically with updated performance of bandit for each pull \n",
    "        \"\"\"\n",
    "        self.arms_counter, self.cumulative_rewards = self.update_reward()\n",
    "        # update arms performance history \n",
    "        for arm in range(self.num_arms):\n",
    "            if self.arms_counter[arm] != 0:\n",
    "                self.arms_performance[arm] = self.cumulative_rewards[arm] / self.arms_counter[arm]\n",
    "        # print(f\"The arms_performance: {self.arms_performance}\")\n",
    "        \n",
    "        # pure exploit\n",
    "        greedy_choice = np.argmax(self.arms_performance)  # the arm with the most mean reward\n",
    "        # if there is a tie for current best policy\n",
    "        # collect all the arm indices with \n",
    "        ties = [index for index, value in enumerate(self.arms_performance) if value == greedy_choice]\n",
    "        if len(ties) > 1:\n",
    "            greedy_choice = np.random.choice(ties)  # break the tie randomly\n",
    "        return greedy_choice\n",
    "    \n",
    "    def select_arm(self):\n",
    "        \"\"\" Choose arms based on Epsilon-greedy:\n",
    "            Apply:\n",
    "                Random Policy -- explore -- RandomPolicy()\n",
    "            or\n",
    "                greedy policy -- exploit -- self.greedy_mode()\n",
    "        \"\"\"\n",
    "        rand_flag = np.random.rand()\n",
    "        # print(f\"The random number for this time: {rand_flag}\")\n",
    "        if rand_flag < self.eps:\n",
    "            self.arms_counter, self.cumulative_rewards = self.update_reward()  # update reward information\n",
    "            self.current_arm = RandomPolicy(self.num_arms).select_arm()  # apply explore\n",
    "        elif rand_flag > self.eps:\n",
    "            self.current_arm = self.greedy_mode()  # apply exploit\n",
    "        else:  # in case rand_flag = self.eps\n",
    "            policies = [RandomPolicy(self.num_arms).select_arm(), self.greedy_mode()]\n",
    "            choice_index = np.random.choice([0,1])\n",
    "            self.current_arm = policies[choice_index]\n",
    "        self.t += 1\n",
    "        return self.current_arm\n",
    "        \n",
    "    def update_reward(self):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        # print(f\"The selected arm from last Epsilon-greedy: {self.current_arm}\")\n",
    "        if self.current_arm == None:\n",
    "            return self.arms_counter, self.cumulative_rewards\n",
    "        \n",
    "        global CurRewardList_EGP\n",
    "        current_reward = CurRewardList_EGP[self.t-1]  # receive current reward from global scope \n",
    "        # print(f\"The reward of last pull: {current_reward}\")\n",
    "        self.arms_counter[self.current_arm] += 1  # update known-arms space\n",
    "        self.cumulative_rewards[self.current_arm] += current_reward  # update rewards histroy\n",
    "        # print(f\"The currect cumulative rewards: {self.cumulative_rewards}\")  # initialized with 0s\n",
    "        # print(f\"The currect count of arm space: {self.arms_counter}\")\n",
    "        return self.arms_counter, self.cumulative_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[360.0, 53.0], [355.0, 32.0], [364.0, 29.0], [418.0, 42.0], [369.0, 28.0], [372.0, 31.0], [371.0, 31.0], [367.0, 29.0], [360.0, 39.0], [375.0, 44.0]]\n",
      "Time Usage : 0.6622772216796875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "203.45"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" DEMO\n",
    "\"\"\"\n",
    "t1 = time.time()\n",
    "CurRewardList_EGP = []\n",
    "num_arms = SimpleBandit().num_arms\n",
    "rp = RandomPolicy(num_arms)\n",
    "eg = Epsilon_Greedy_Policy(num_arms, 0.1)\n",
    "policies = [rp, eg]\n",
    "regrets = [run_trajectory(SimpleBandit(), policies, 1000) for _ in range(10)]\n",
    "print(regrets)\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CurRewardList_EGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Usage : 7.867813110351562e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/furinkazan/Library/Python/3.7/lib/python/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/furinkazan/Library/Python/3.7/lib/python/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code\n",
    "t1 = time.time()\n",
    "regrets = []\n",
    "\n",
    "# Your code here, \n",
    "    # generate 10 regrets with simple bandit and epsilon-greedy policy, \n",
    "    # save results in regrets,\n",
    "    # refer to second block in exercise 1\n",
    "\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Exp3 is a policy that is more nuanced.  The idea is that at each time point you pull an arm with some probability `pi[i]` which is updated based on the performance of the arm.  Look at the full algorithm in http://proceedings.mlr.press/v24/seldin12a/seldin12a.pdf, Algorithm 1.  \n",
    "\n",
    "Implement this version of the Exp3 algorithm in the following class, test it by having it compete once with the random policy for T = 1000 time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp3:\n",
    "    \"\"\" \n",
    "    Exp3 algorithm:\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.current_arm = None\n",
    "        self.t = 0  # initialized time step \n",
    "        self.weights = np.ones(self.num_arms)  # initialize weights to be 1\n",
    "        self.prob_dist = np.zeros(self.num_arms)  # initialize probability distributions to be 0\n",
    "        \n",
    "        \n",
    "    def choose_rate(self):\n",
    "        \"\"\"  \n",
    "        Choose the learning rate Îµt:\n",
    "            which is equal to the exploration rate, is changing with time\n",
    "        \"\"\"\n",
    "        epsi_t = min(1. / self.num_arms, np.sqrt(np.log(self.num_arms) / (self.t * self.num_arms)))\n",
    "        return epsi_t\n",
    "    \n",
    "    def draw(self,):\n",
    "        \"\"\" set distribution \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        total_weights = sum(self.weights)\n",
    "        epsi_t = self.choose_rate()\n",
    "        for arm in range(self.num_arms):\n",
    "            # update probability distributions\n",
    "            self.prob_dist[arm] = (1 - epsi_t) * (self.weights[arm] / total_weights) + epsi_t / self.num_arms\n",
    "        \n",
    "        self.t += 1\n",
    "        return \n",
    "    \n",
    "    def update_reward(self, reward):\n",
    "        None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distr(weights, gamma=0.0):\n",
    "    theSum = float(sum(weights))\n",
    "    return tuple((1.0 - gamma) * (w / theSum) + (gamma / len(weights)) for w in weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Usage : 7.295608520507812e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code\n",
    "t1 = time.time()\n",
    "regrets = []\n",
    "\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** The Upper Confidence Bound (UCB) algorithm is intuitive.  You pull the arm that has the largest upper confidence bound for the mean reward.  So if you have an upper bound on the mean reward for an arm this is either because the arm is performing well or you have not pulled the arm much, resulting in a wide confidence interval.  The UCB for arm i is then,\n",
    "$$ U_{i,t} = \\hat \\mu_{i,t} + C \\sqrt{\\frac{\\log(t)}{n_{i,t} + 1}}$$\n",
    "where $\\hat \\mu_{i,t}$ is the mean reward of the arm up to time $t$, $n_{i,t}$ is the number of times that that arm has been pulled.  $C$ is an argument (can be taken to be 2 by default).\n",
    "\n",
    "Implement this version of the UCB algorithm in the following class, test it by having it compete once with the random policy for T = 1000 time points with $C = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound (UCB) algorithm:\n",
    "        pull the arm that has the largest upper confidence bound for the mean reward\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms, C):\n",
    "        self.num_arms = num_arms  # total number of arms \n",
    "        self.current_arm = None\n",
    "        self.C = C\n",
    "        self.t = 0  # initialized time step \n",
    "        self.arms_counter = np.zeros(self.num_arms)\n",
    "        self.arms_performance = np.zeros(self.num_arms)  # update arms mean rewards dynamically \n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\" \n",
    "        Apply UCB decision rule \n",
    "        \"\"\"\n",
    "        self.arms_counter, self.arms_performance = self.update_reward()\n",
    "        if self.current_arm == None:\n",
    "            self.current_arm = RandomPolicy(self.num_arms).select_arm()  # tie for the first pull \n",
    "            return self.current_arm\n",
    "        pull_sum = sum(self.arms_counter)\n",
    "        u_space = [self.arms_performance[a] + \n",
    "                   np.sqrt((self.C * np.log(pull_sum)) / (self.arms_counter[a] + 1)) \n",
    "                   for a in range(self.num_arms)]\n",
    "        self.current_arm = np.argmax(u_space)\n",
    "        \n",
    "        # if there is a tie for current best policy\n",
    "        ties = [index for index, value in enumerate(self.arms_performance) if value == self.current_arm]\n",
    "        if len(ties) > 1:\n",
    "            self.current_arm = np.random.choice(ties)  # break the tie randomly\n",
    "        self.t += 1\n",
    "        return self.current_arm\n",
    "        \n",
    "    def update_reward(self):\n",
    "        \"\"\" \n",
    "        Update:\n",
    "            1. arm space\n",
    "            2. arm mean performance \n",
    "        \"\"\"\n",
    "        if self.current_arm == None:\n",
    "            return self.arms_counter, self.arms_performance  # exit for the first pull \n",
    "        global CurRewardList_UCB\n",
    "        current_reward = CurRewardList_UCB[self.t-1]  # receive current reward from global scope\n",
    "        self.arms_counter[self.current_arm] += 1  # update known-arms space\n",
    "        # update rewards histroy\n",
    "        self.arms_performance[self.current_arm] += (\n",
    "            current_reward - self.arms_performance[self.current_arm]) / self.arms_counter[self.current_arm]   \n",
    "        return self.arms_counter, self.arms_performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[355.0, 190.0], [389.0, 71.0], [372.0, 45.0], [374.0, 47.0], [358.0, 28.0], [421.0, 19.0], [422.0, 22.0], [358.0, 21.0], [396.0, 18.0], [352.0, 14.0]]\n",
      "Time Usage : 1.1155729293823242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "213.6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" DEMO Ex 4\n",
    "\"\"\"\n",
    "t1 = time.time()\n",
    "CurRewardList_UCB= []\n",
    "num_arms = SimpleBandit().num_arms\n",
    "rp = RandomPolicy(num_arms)\n",
    "ucb = UCB(num_arms, 2)\n",
    "policies = [rp, ucb]\n",
    "regrets = [run_trajectory(SimpleBandit(), policies, 1000)  for _ in range(10)]\n",
    "print(regrets)\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(CurRewardList_UCB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Usage : 4.673004150390625e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code\n",
    "t1 = time.time()\n",
    "regrets = []\n",
    "\n",
    "\n",
    "print('Time Usage : {}'.format(time.time() - t1))\n",
    "np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** \n",
    "\n",
    "1. Modify the `run_trajectory` def to output all of the regrets up to that time in a T x K (for K policies) array.\n",
    "2. Try 4 different values of $\\epsilon$ for epsilon-greedy: 0.01, 0.05, 0.1, 0.2 and have them compete.  Plot the regrets as a function of t.  Note the best performing selection of $\\epsilon$ at $T = 1000$.\n",
    "3. Try 4 different values of $C$ in UCB: 0.5, 0.75, 1, 2 and have them compete.  Plot the regrets as a function of t.  Note the best performing selection of $C$ at $T = 1000$.\n",
    "4. Using the optimal values of $C$ and $\\epsilon$ have all four methods compete. Plot the regrets as a function of t, remark on if the relative performance changes over time.  Is one algorithm always dominant?  Make any other conclusions.\n",
    "\n",
    "**Note**: \n",
    "  + Please finishe 4 parts in 4 code blocks.\n",
    "  + For part 2, 3, and 4 you should call `run_trajectory` in each of them. In the end of each part, you need to write a markdown block to explain your findings and state your conclusion.\n",
    "  + For each part, set figure size to (17, 8), plot all lines in the same graph and set proper title. DO NOT create more than 1 subplots. Include legend in your plots so that they are easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1. Write run_trajectory function here again, do not modify the above block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 code, output your plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings and conclusion for part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 code, output your plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings and conclusion for part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 code, output your plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings and conclusion for part 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
